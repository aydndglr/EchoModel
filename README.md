# EchoModel: GeliÅŸmiÅŸ, ModÃ¼ler BÃ¼yÃ¼k Dil Modeli

<p align="center">
  <img src="https://img.shields.io/badge/Status-In%20Progress-blue" alt="Project Status">
  <img src="https://img.shields.io/badge/Built%20With-PyTorch-red" alt="Built With PyTorch">
  <img src="https://img.shields.io/badge/Contributors-aydndglr" alt="Contributors">
</p>

---

## ğŸš€ Projeye Genel BakÄ±ÅŸ

**EchoModel**, PyTorch kullanarak sÄ±fÄ±rdan inÅŸa edilen, modÃ¼ler ve geniÅŸletilebilir bir **Decoder-Only Transformer** tabanlÄ± bÃ¼yÃ¼k dil modelidir (LLM). AmacÄ±m, OLMO, Gemma veya GPT gibi modern LLM'lerin temel prensiplerini takip ederek, ancak kendi Ã¶zel mimarim ile esnek veri iÅŸleme yeteneklerine sahip, yÃ¼ksek performanslÄ± bir model geliÅŸtirmektir. Proje yapÄ±sÄ±, araÅŸtÄ±rma ve geliÅŸtirme iÃ§in saÄŸlam bir temel sunarken, gelecekte potansiyel olarak harici bellek entegrasyonu gibi yenilikÃ§i Ã¶zelliklere de kapÄ± aralamaktadÄ±r.

Bu proje, Ã¶zellikle dil modelleme ve metin Ã¼retimi alanÄ±nda derinlemesine bilgi edinmek, kendi LLM'inizi baÅŸtan sona inÅŸa etmek ve Ã¶zelleÅŸtirmek isteyen geliÅŸtiriciler ve araÅŸtÄ±rmacÄ±lar iÃ§in bir referans noktasÄ± olmayÄ± hedeflemektedir.

---

## âœ¨ Temel Ã–zellikler ve YaklaÅŸÄ±m

* **Decoder-Only Transformer Mimarisi:** Metin Ã¼retme (text generation) gÃ¶revleri iÃ§in optimize edilmiÅŸ, modern Transformer mimarisinin temelini kullanÄ±rÄ±z.
* **Tamamen ModÃ¼ler TasarÄ±m:** Her bir bileÅŸen (gÃ¶mme katmanlarÄ±, dikkat mekanizmalarÄ±, Transformer bloklarÄ±, veri iÅŸleme, eÄŸitim dÃ¶ngÃ¼sÃ¼ vb.) ayrÄ± modÃ¼ller halinde tasarlanmÄ±ÅŸtÄ±r. Bu, kodun okunabilirliÄŸini, bakÄ±mÄ±nÄ± ve yeniden kullanÄ±labilirliÄŸini artÄ±rÄ±r.
* **Esnek Veri Ä°ÅŸleme Pipeline'Ä±:** Wikipedia, Hugging Face `datasets`, Alpaca formatÄ± gibi Ã§eÅŸitli veri kaynaklarÄ±nÄ± iÅŸleyebilecek genel bir veri Ã¶n iÅŸleme ve yÃ¼kleme sistemi.
* **Ã–zelleÅŸtirilebilir Tokenizer:** Kendi veri setlerimiz Ã¼zerinde eÄŸitebileceÄŸimiz Byte-Pair Encoding (BPE) veya karakter tabanlÄ± tokenleyiciler iÃ§in destek.
* **Temiz EÄŸitim ve Ã‡Ä±karÄ±m Ã‡erÃ§evesi:** SaÄŸlam bir eÄŸitim dÃ¶ngÃ¼sÃ¼ (`Trainer` sÄ±nÄ±fÄ±) ve eÄŸitilmiÅŸ modellerle metin Ã¼retme (`Generator`) ve tahmin yapma (`Predictor`) araÃ§larÄ±.
* **AÅŸamalÄ± GeliÅŸtirme Stratejisi:** Ä°lk aÅŸamada saÄŸlam bir temel Transformer modeli oluÅŸturup, kazanÄ±lan tecrÃ¼be ile gelecekte "RAM tabanlÄ± dinamik bellek" entegrasyonu gibi daha ileri seviye Ã¶zelliklere geÃ§iÅŸ ve dinamik eÄŸitim iÃ§in altyapÄ± yapÄ± iÃ§in Ã§alÄ±ÅŸma.

---

## ğŸ› ï¸ Kurulum

Projeyi yerel makinenizde Ã§alÄ±ÅŸtÄ±rmak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin:

1.  **Projeyi KlonlayÄ±n:**
    ```bash
    git clone https://github.com/aydndglr/EchoModel.git
    cd EchoModel
    ```

2.  **Python OrtamÄ± OluÅŸturun (Ã–nerilen: Conda veya venv):**
    ```bash
    # Conda ile
    conda create -n echomodel python=3.9
    conda activate echomodel

    # veya venv ile
    python -m venv .venv
    source .venv/bin/activate # Linux/macOS
    # .venv\Scripts\activate   # Windows
    ```

3.  **BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **CUDA/cuDNN Kurulumu (GPU KullanÄ±mÄ± Ä°Ã§in):**
    EÄŸer NVIDIA GPU kullanÄ±yorsanÄ±z, PyTorch'un GPU desteÄŸi iÃ§in CUDA Toolkit ve cuDNN'in doÄŸru sÃ¼rÃ¼mlerini yÃ¼klemeniz gerekmektedir. PyTorch resmi web sitesindeki kurulum yÃ¶nergelerini (https://pytorch.org/get-started/locally/) takip edin.

---

## ğŸš€ KullanÄ±m

### 1. Veri HazÄ±rlÄ±ÄŸÄ±

* Ham veri setlerinizi (Ã¶rn. Wikipedia dump'Ä±, Alpaca JSON'larÄ±) `data/raw/` klasÃ¶rÃ¼ altÄ±na uygun alt klasÃ¶rlerde (Ã¶rn. `data/raw/wikipedia/`, `data/raw/alpaca/`) yerleÅŸtirin.
* Veriyi iÅŸlemek ve `processed/` klasÃ¶rÃ¼ne kaydetmek iÃ§in `preprocess_data.py` betiÄŸini kullanÄ±n.
    ```bash
    python src/scripts/preprocess_data.py --config configs/data_config.yaml
    ```
    *(`data_config.yaml` dosyasÄ±nÄ± veri setinizin yol ve format bilgileriyle gÃ¼ncellemeniz gerekecektir.)*

### 2. Tokenizer EÄŸitimi

* Ä°ÅŸlenmiÅŸ veri Ã¼zerinde kendi tokenizer'Ä±nÄ±zÄ± eÄŸitmek iÃ§in `train_tokenizer.py` betiÄŸini Ã§alÄ±ÅŸtÄ±rÄ±n. EÄŸitilen tokenizer varlÄ±klarÄ± `tokenizer_assets/` altÄ±na kaydedilecektir.
    ```bash
    python src/scripts/train_tokenizer.py --config configs/tokenizer_config.yaml
    ```
    *(Bu config dosyasÄ± henÃ¼z oluÅŸturulmadÄ±, gerektiÄŸinde eklenecektir.)*

### 3. Model EÄŸitimi

* Modelin hiperparametrelerini `configs/model_config.yaml` ve eÄŸitim ayarlarÄ±nÄ± `configs/training_config.yaml` dosyalarÄ±nda dÃ¼zenleyin.
* Modeli eÄŸitmek iÃ§in `train_model.py` betiÄŸini Ã§alÄ±ÅŸtÄ±rÄ±n:
    ```bash
    python src/main.py --task train --model_config configs/model_config.yaml --train_config configs/training_config.yaml --data_config configs/data_config.yaml
    ```
    *(Veya doÄŸrudan `python src/scripts/train_model.py` gibi bir betik yazabiliriz.)*
* EÄŸitilmiÅŸ model aÄŸÄ±rlÄ±klarÄ± `saved_models/` dizinine kaydedilecektir. EÄŸitim loglarÄ± ve metrikler `logs/` dizininde bulunacaktÄ±r (TensorBoard ile izlenebilir).

### 4. Metin Ãœretimi (Inference)

* EÄŸitilmiÅŸ bir modeli yÃ¼kleyin ve metin Ã¼retimi yapmak iÃ§in `generate_text.py` betiÄŸini kullanÄ±n:
    ```bash
    python src/main.py --task generate --checkpoint_path saved_models/your_model.pt --prompt "Your starting text here."
    ```
    *(Veya doÄŸrudan `python src/inference/generator.py` gibi bir betik yazabiliriz.)*

---

## ğŸ¤ KatkÄ±da Bulunma

Proje, aÃ§Ä±k kaynaklÄ±dÄ±r ve katkÄ±lara aÃ§Ä±ktÄ±r. Her tÃ¼rlÃ¼ katkÄ± (hata dÃ¼zeltmeleri, yeni Ã¶zellikler, dokÃ¼mantasyon iyileÅŸtirmeleri, model eÄŸitimi iÃ§in teknolojk altyapÄ± vb.) memnuniyetle karÅŸÄ±lanÄ±r. LÃ¼tfen bir katkÄ±da bulunmadan Ã¶nce [CONTRIBUTING.md](CONTRIBUTING.md) dosyasÄ±nÄ± (ileride eklenecektir) inceleyin.

---

## ğŸ“œ Lisans

Bu proje, **Apache 2.0** altÄ±nda lisanslanmÄ±ÅŸtÄ±r ayrÄ±ca ticari kullanim icin ek sartlar bulunmaktadÄ±r . Daha fazla bilgi iÃ§in `LICENSE` dosyasÄ±na bakÄ±n.

---

## ğŸ“§ Ä°letiÅŸim

SorularÄ±nÄ±z, geri bildirimleriniz veya iÅŸbirliÄŸi iÃ§in lÃ¼tfen [aydin.daglar@outlook.com](mailto:aydin.daglar@outlook.com) adresinden iletiÅŸime geÃ§in.

---
