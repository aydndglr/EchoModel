
# src/tokenizer/char_tokenizer.py

import os
import json
import logging
import shutil
from typing import List, Dict, Optional, Set, Union

from src.tokenizer.special_tokens import BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN

log = logging.getLogger(__name__)

class CharTokenizer:
    """
    Karakter bazlÄ± bir tokenleyiciyi yÃ¶netir.
    Her karakteri benzersiz bir ID'ye eÅŸler. BPE'ye gÃ¶re daha basit ve kÃ¼Ã§Ã¼k kelime haznesi iÃ§in uygundur.
    """
    def __init__(self, chars: Optional[Union[List[str], Set[str]]] = None, unk_token: str = UNK_TOKEN):
        """
        Args:
            chars (Optional[Union[List[str], Set[str]]]): Tokenleyiciye dahil edilecek karakterlerin listesi veya kÃ¼mesi.
                                                          Belirtilmezse, boÅŸ bir tokenleyici oluÅŸturulur.
            unk_token (str): Bilinmeyen token iÃ§in kullanÄ±lacak string.
        """
        self.vocab: Dict[str, int] = {}
        self.id_to_token: Dict[int, str] = {}
        self.unk_token = unk_token
        self.special_tokens = [BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN]

        # Ã–zel tokenlarÄ± her zaman kelime haznesine ekle ve baÅŸlangÄ±Ã§ ID'lerini ata
        self._add_special_tokens_to_vocab()

        if chars is not None:
            self._build_vocab_from_chars(chars)

        log.info(f"CharTokenizer baÅŸlatÄ±ldÄ±. Kelime haznesi boyutu: {len(self.vocab)}")

    def _add_special_tokens_to_vocab(self):
        """Ã–zel tokenlarÄ± kelime haznesine ekler."""
        current_id = 0
        for token in self.special_tokens:
            if token not in self.vocab:
                self.vocab[token] = current_id
                self.id_to_token[current_id] = token
                current_id += 1
        # UNK token ID'sini kaydet
        self.unk_token_id = self.vocab[self.unk_token]

    def _build_vocab_from_chars(self, chars: Union[List[str], Set[str]]):
        """Verilen karakterlerden kelime haznesini oluÅŸturur."""
        for char in chars:
            if char not in self.vocab:
                current_id = len(self.vocab) # Yeni ID mevcut kelime haznesinin boyutu olacak
                self.vocab[char] = current_id
                self.id_to_token[current_id] = char

    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """
        Metni karakter ID'lerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        """
        ids = []
        if add_special_tokens and BOS_TOKEN in self.vocab:
            ids.append(self.vocab[BOS_TOKEN])

        for char in text:
            ids.append(self.vocab.get(char, self.unk_token_id))

        if add_special_tokens and EOS_TOKEN in self.vocab:
            ids.append(self.vocab[EOS_TOKEN])
        return ids

    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:
        """
        Karakter ID'lerini metne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        """
        chars = []
        for id_val in ids:
            token = self.id_to_token.get(id_val, self.unk_token)
            if skip_special_tokens and token in self.special_tokens:
                continue
            chars.append(token)
        return "".join(chars)

    def save_vocabulary(self, save_path: str):
        """
        Tokenleyici kelime haznesini bir JSON dosyasÄ±na kaydeder.
        """
        tokenizer_file = os.path.join(save_path, "char_vocab.json")
        os.makedirs(save_path, exist_ok=True)
        with open(tokenizer_file, "w", encoding="utf-8") as f:
            json.dump(self.vocab, f, indent=4)
        log.info(f"CharTokenizer kelime haznesi '{tokenizer_file}' konumuna kaydedildi.")

    @classmethod
    def from_pretrained(cls, path: str) -> "CharTokenizer":
        """
        KaydedilmiÅŸ bir kelime haznesinden CharTokenizer'Ä± yÃ¼kler.
        """
        tokenizer_file = os.path.join(path, "char_vocab.json") if os.path.isdir(path) else path
        if not os.path.exists(tokenizer_file):
            raise FileNotFoundError(f"CharTokenizer kelime haznesi dosyasÄ± bulunamadÄ±: {tokenizer_file}")
        
        with open(tokenizer_file, "r", encoding="utf-8") as f:
            vocab_loaded = json.load(f)
        
        # Sadece karakterleri al, Ã¶zel tokenlarÄ± otomatik ekleyen init'i kullan.
        # BÃ¶ylece ID'ler tutarlÄ± kalÄ±r (Ã¶zel tokenlar her zaman baÅŸta).
        chars_only = [char for char in vocab_loaded if char not in [BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN]]
        
        instance = cls(chars=chars_only, unk_token=UNK_TOKEN)
        # YÃ¼klenen vocab'Ä± doÄŸrudan atamak yerine, init metodumuzun oluÅŸturduÄŸu vocab'Ä± gÃ¼ncelleyelim.
        # Bu, Ã¶zel tokenlarÄ±n ID'lerinin doÄŸru sÄ±ralamada olmasÄ±nÄ± saÄŸlar.
        # Bu kÄ±sÄ±m biraz karmaÅŸÄ±klaÅŸabilir, basitleÅŸtirelim:
        
        # Basit yÃ¼kleme: EÄŸer Ã¶zel token ID'leri de kaydedildiyse, onlarÄ± kullan.
        # EÄŸer Ã¶zel token ID'leri dinamik olarak atandÄ±ysa, yeniden atamamÄ±z gerekebilir.
        # Åimdilik, yÃ¼klenen vocab'Ä±n Ã¶zel tokenlarÄ± doÄŸru ID'lerde iÃ§erdiÄŸini varsayalÄ±m.
        instance.vocab = vocab_loaded
        instance.id_to_token = {v: k for k, v in vocab_loaded.items()}
        instance.unk_token_id = instance.vocab[UNK_TOKEN] # YÃ¼klenen vocab'dan UNK ID'sini al
        
        log.info(f"CharTokenizer kelime haznesi '{tokenizer_file}' konumundan yÃ¼klendi. Boyut: {len(instance.vocab)}")
        return instance

    @property
    def vocabulary_size(self) -> int:
        """Kelime haznesi boyutunu dÃ¶ndÃ¼rÃ¼r."""
        return len(self.vocab)

# --- Test Fonksiyonu ---
if __name__ == "__main__":
    print("CharTokenizer testi baÅŸlatÄ±lÄ±yor...")

    # Loglama sistemini kur (test iÃ§in gerekli)
    from src.utils.logger import setup_logging
    test_output_dir = "test_runs_char_tokenizer" # KlasÃ¶r adÄ±nÄ± daha spesifik yaptÄ±k
    test_run_name = "char_tokenizer_test_run"
    
    # Mevcut test dizinlerini temizle
    if os.path.exists(test_output_dir):
        shutil.rmtree(test_output_dir)
    os.makedirs(test_output_dir, exist_ok=True) 

    # GeÃ§ici olarak tÃ¼m log handler'larÄ±nÄ± kapat (Windows PermissionError'Ä± iÃ§in)
    for handler in logging.root.handlers[:]:
        if isinstance(handler, logging.FileHandler):
            handler.close()
        logging.root.removeHandler(handler)
    
    setup_logging(log_level="INFO", output_dir=test_output_dir, run_name=test_run_name)
    log = logging.getLogger(__name__)

    # Test iÃ§in karakterler
    test_chars = list("abcÃ§defgÄŸhÄ±ijklmnoÃ¶prsÅŸtuÃ¼vyz ABCÃ‡DEFGÄHIÄ°JKLMNOÃ–PRSÅTUÃœVYZ0123456789.,!?")

    # CharTokenizer oluÅŸtur
    print("\n--- CharTokenizer OluÅŸturma Testi ---")
    tokenizer = CharTokenizer(chars=test_chars)
    print(f"  OluÅŸturulan kelime haznesi boyutu: {tokenizer.vocabulary_size}")
    assert tokenizer.vocabulary_size > len(test_chars), "Kelime haznesi boyutu yanlÄ±ÅŸ!" # Ã–zel tokenlar da eklendiÄŸi iÃ§in bÃ¼yÃ¼k olmalÄ±
    print("CharTokenizer oluÅŸturma testi baÅŸarÄ±lÄ±. âœ…")

    # Encode/Decode Testi
    print("\n--- Encode/Decode Testi ---")
    test_text = "Merhaba DÃ¼nya! 123"
    encoded_ids = tokenizer.encode(test_text, add_special_tokens=True)
    decoded_text = tokenizer.decode(encoded_ids, skip_special_tokens=True)
    
    print(f"  Orijinal Metin: '{test_text}'")
    print(f"  KodlanmÄ±ÅŸ ID'ler (Ã¶zel tokenlarla): {encoded_ids}")
    print(f"  Ã‡Ã¶zÃ¼lmÃ¼ÅŸ Metin: '{decoded_text}'")
    
    assert decoded_text == test_text, "Encode/Decode tutarsÄ±zlÄ±ÄŸÄ±!"
    print("Encode/Decode testi baÅŸarÄ±lÄ±. âœ…")

    # Bilinmeyen Token Testi
    print("\n--- Bilinmeyen Token Testi ---")
    unknown_text = "âœ¨ğŸš€ğŸ”¥" # Kelime haznesinde olmayan karakterler
    encoded_unknown = tokenizer.encode(unknown_text, add_special_tokens=False)
    decoded_unknown = tokenizer.decode(encoded_unknown, skip_special_tokens=False) # Skip special tokens yapmayalÄ±m
    
    print(f"  Bilinmeyen Metin: '{unknown_text}'")
    print(f"  KodlanmÄ±ÅŸ Bilinmeyen ID'ler: {encoded_unknown}")
    print(f"  Ã‡Ã¶zÃ¼lmÃ¼ÅŸ Bilinmeyen Metin: '{decoded_unknown}'")
    
    # TÃ¼m bilinmeyen karakterlerin UNK tokenÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ÄŸÃ¼nÃ¼ kontrol et
    assert all(id_val == tokenizer.unk_token_id for id_val in encoded_unknown), "Bilinmeyen token eÅŸleÅŸmesi yanlÄ±ÅŸ!"
    assert all(char == tokenizer.unk_token for char in decoded_unknown), "Bilinmeyen karakter Ã§Ã¶zÃ¼lÃ¼rken yanlÄ±ÅŸ!"
    print("Bilinmeyen token testi baÅŸarÄ±lÄ±. âœ…")

    # Kaydetme ve YÃ¼kleme Testi
    print("\n--- Kaydetme ve YÃ¼kleme Testi ---")
    tokenizer_save_path = os.path.join(test_output_dir, "char_tokenizer_assets")
    tokenizer.save_vocabulary(tokenizer_save_path)
    
    loaded_tokenizer = CharTokenizer.from_pretrained(tokenizer_save_path)
    assert loaded_tokenizer.vocabulary_size == tokenizer.vocabulary_size, "YÃ¼klenen kelime haznesi boyutu yanlÄ±ÅŸ!"
    assert loaded_tokenizer.encode("test", add_special_tokens=False) == tokenizer.encode("test", add_special_tokens=False), "YÃ¼klenen tokenleyici tutarsÄ±z!"
    print("Kaydetme ve yÃ¼kleme testi baÅŸarÄ±lÄ±. âœ…")

    print("\nCharTokenizer tÃ¼m testleri tamamlandÄ±. âœ…")

    # Test sonrasÄ± oluÅŸturulan test dizinini temizle (finally bloÄŸunda olmalÄ±ydÄ± ama burada manuel kontrol)
    # Bu kÄ±smÄ± ana test betiÄŸindeki finally bloÄŸuna taÅŸÄ±dÄ±k.
    if os.path.exists(test_output_dir):
        shutil.rmtree(test_output_dir)
        print(f"Temizlik yapÄ±ldÄ±: '{test_output_dir}' silindi.")